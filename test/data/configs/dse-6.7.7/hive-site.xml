<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!--
   This file is managed by DataStax OpsCenter LifeCycle Manager.
   Manual edits will be overwritten by the next install or configure
   job that runs on this system.
-->

<configuration>

    <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
    <!-- that are implied by Hadoop setup variables.                                                -->
    <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
    <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
    <!-- resource).                                                                                 -->

    <!-- Hive Execution Parameters -->
    <property>
        <name>hive.exec.mode.local.auto</name>
        <value>false</value>
        <description>Let hive determine whether to run in local mode automatically</description>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <!-- Use dsefs file system, set it to  dsefs://127.0.0.1:5598/user/spark/warehouse -->
        <value>dsefs:///user/spark/warehouse</value>
        <description>location of default database for the warehouse</description>
    </property>

    <property>
        <name>hive.metastore.rawstore.impl</name>
        <value>com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStore</value>
        <description>Use the Apache Cassandra Hive RawStore implementation</description>
    </property>

    <property>
      <name>com.datastax.bdp.fs.client.authentication.factory</name>
      <value>com.datastax.bdp.fs.hadoop.DseRestClientAuthProviderBuilderFactory</value>
    </property>

    <!-- Set this to true to enable auto-creation of Cassandra keyspaces as Hive Databases -->
    <property>
        <name>cassandra.autoCreateHiveSchema</name>
        <value>true</value>
    </property>

    <property>
        <name>spark.enable</name>
        <value>true</value>
    </property>

    <property>
        <name>cassandra.connection.metaStoreColumnFamilyName</name>
        <value>sparkmetastore</value>
    </property>

    <!-- https://issues.apache.org/jira/browse/HIVE-8529 -->
    <property>
      <name>hive.server2.logging.operation.enabled</name>
      <value>false</value>
    </property>

    <property>
      <name>cassandra.port</name>
      <value>${cassandra.connection.native.port}</value>
    </property>

  <!--There are three possible JDBC authentication setups for Spark SQL Thrift Server.-->
  <!--Uncomment one section at a time to use the selected authentication setup.-->

  <!-- Start of: configuration for not authenticating JDBC users -->
  
  <!-- End of: configuration for not authenticating JDBC users -->

  <!-- Start of: configuration for authenticating JDBC users with Kerberos -->
  <!--
    <property>
      <name>hive.server2.enable.doAs</name>
      <value>true</value>
    </property>

    <property>
      <name>hive.server2.authentication</name>
      <value>KERBEROS</value>
    </property>

    <property>
      <name>hive.server2.authentication.kerberos.principal</name>
      <value>hiveserver2/_HOST@EXAMPLE.COM</value>
    </property>

    <property>
      <name>hive.server2.authentication.kerberos.keytab</name>
      <value>/path/to/hiveserver2.keytab</value>
    </property>
  -->
  <!-- End of: configuration for authenticating JDBC users with Kerberos -->

  <!-- Start of: configuration for authenticating JDBC users with plain text password -->
  
  <!-- End of: configuration for authenticating JDBC users with plain text password -->

  
  <!-- SSL options for SparkSQL -->
    <property>
        <name>hive.server2.use.SSL</name>
        <value>false</value>
    </property>
    

    <property>
        <name>hive.server2.transport.mode</name>
        <value>binary</value>
    </property>

    <property>
        <name>spark.hadoop.com.datastax.bdp.fs.client.authentication.factory</name>
        <value>com.datastax.bdp.fs.hadoop.DseRestClientAuthProviderBuilderFactory</value>
    </property>
</configuration>
